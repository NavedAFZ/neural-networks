{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiclass_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCsXbmkJYwX1AYTVLI5ymv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NavedAFZ/neural-networks/blob/master/multiclass_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK0XSlOlFl-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import vstack\n",
        "from pandas import read_csv\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torch import Tensor\n",
        "from torch.nn import Linear\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import Sigmoid\n",
        "from torch.nn import Module\n",
        "from torch.optim import SGD\n",
        "from torch.nn import BCELoss\n",
        "from torch.nn.init import kaiming_uniform_\n",
        "from torch.nn.init import xavier_uniform_\n",
        "from torch.nn import Softmax\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from numpy import vstack\n",
        "from numpy import argmax"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg4HxHTSFsIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CSV_data(Dataset):\n",
        "    # load the dataset\n",
        "    def __init__(self, path):\n",
        "        # load the csv file as a dataframe\n",
        "        df = read_csv(path, header=None)\n",
        "        # store the inputs and outputs\n",
        "        self.X = df.values[:, :-1]\n",
        "        self.y = df.values[:, -1]\n",
        "        # ensure input data is floats\n",
        "        self.X = self.X.astype('float32')\n",
        "        # label encode target and ensure the values are floats\n",
        "        self.y = LabelEncoder().fit_transform(self.y)\n",
        "        #self.y = self.y.astype('float32')\n",
        "        #self.y = self.y.reshape((len(self.y), 1))\n",
        " \n",
        "    # number of rows in the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        " \n",
        "    # get a row at an index\n",
        "    def __getitem__(self, idx):\n",
        "        return [self.X[idx], self.y[idx]]\n",
        " \n",
        "    # get indexes for train and test rows\n",
        "    def __split__(self, n_test=0.33):\n",
        "        # determine sizes\n",
        "        test_size = round(n_test * len(self.X))\n",
        "        train_size = len(self.X) - test_size\n",
        "        # calculate the split\n",
        "        return random_split(self, [train_size, test_size])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cm4J5vUvI8py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class my_model(Module):\n",
        "  def __init__(self,n_inputs):\n",
        "    super(my_model,self).__init__()\n",
        "    self.hidden1=Linear(n_inputs,10)\n",
        "    kaiming_uniform_(self.hidden1.weight,nonlinearity='relu')\n",
        "    self.act1=ReLU()\n",
        "    self.hidden2=Linear(10,8)\n",
        "    kaiming_uniform_(self.hidden2.weight,nonlinearity='relu')\n",
        "    self.act2=ReLU()\n",
        "    self.hidden3=Linear(8,3)\n",
        "    xavier_uniform_(self.hidden3.weight)\n",
        "    self.act3=Softmax(dim=1)\n",
        "  def forward(self,X):\n",
        "    X=self.hidden1(X)\n",
        "    X=self.act1(X)\n",
        "    X=self.hidden2(X)\n",
        "    X=self.act2(X)\n",
        "    X=self.hidden3(X)\n",
        "    X=self.act3(X)\n",
        "    return X\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLljxiV3OWO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(path):\n",
        "    # load the dataset\n",
        "    dataset = CSV_data(path)\n",
        "    # calculate split\n",
        "    train, test = dataset.__split__()\n",
        "    # prepare data loaders\n",
        "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
        "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
        "    return train_dl, test_dl  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL7RcTTWOdcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_dl, model):\n",
        "    # define the optimization\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    # enumerate epochs\n",
        "    for epoch in range(100):\n",
        "        # enumerate mini batches\n",
        "        for  data in train_dl:\n",
        "          inputs, targets=data          \n",
        "          # clear the gradients\n",
        "          optimizer.zero_grad()\n",
        "          # compute the model output\n",
        "          yhat = model(inputs)\n",
        "          #yhat=yhat.detach().numpy()\n",
        "          #yhat=argmax(yhat,axis=1)\n",
        "          # calculate loss\n",
        "          loss = criterion(yhat, targets)\n",
        "          # credit assignment\n",
        "          loss.backward()\n",
        "          # update model weights\n",
        "          optimizer.step()      \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37zJzZOSOpqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_model(test_dl, model):\n",
        "    predictions, actuals = list(), list()\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\n",
        "        # evaluate the model on the test set\n",
        "        yhat = model(inputs)\n",
        "        # retrieve numpy array\n",
        "        yhat = yhat.detach().numpy()\n",
        "        yhat=argmax(yhat,axis=1)\n",
        "        actual = targets.numpy()\n",
        "        actual = actual.reshape((len(actual), 1))\n",
        "        # round to class values\n",
        "        yhat = yhat.reshape((len(yhat), 1))\n",
        "        \n",
        "        #yhat = yhat.round()\n",
        "        # store\n",
        "        predictions.append(yhat)\n",
        "        actuals.append(actual)\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HoHZE6NPRR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(model,data):\n",
        "  data1=Tensor([data])\n",
        "  out=model(data1)\n",
        "  out=out.detach().numpy()\n",
        "  ind=argmax(out,axis=1)\n",
        "\n",
        "  return ind"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24F0OnvkP0DJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "cca3c2b7-a237-4baf-de68-2f3e735e0cf1"
      },
      "source": [
        "#path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
        "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv'\n",
        "train11,test11=prepare_data(path)\n",
        "print(len(train11.dataset), len(test11.dataset))\n",
        "mod1=my_model(4)\n",
        "\n",
        "train_model(train11,mod1)\n",
        "acc=evaluate_model(test11,mod1)\n",
        "print(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"train_dl, test_dl = prepare_data(path)\n",
        "print(len(train_dl.dataset), len(test_dl.dataset))\n",
        "# define the network\n",
        "model = MLP(4)\n",
        "# train the model\n",
        "train_model(train_dl, model)\n",
        "# evaluate the model\n",
        "acc = evaluate_model(test_dl, model)\n",
        "print('Accuracy: %.3f' % acc)\n",
        "# make a single prediction\n",
        "row = [5.1,3.5,1.4,0.2]\n",
        "yhat = predict(row, model)\n",
        "print('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\"\"\"\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 50\n",
            "0.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"train_dl, test_dl = prepare_data(path)\\nprint(len(train_dl.dataset), len(test_dl.dataset))\\n# define the network\\nmodel = MLP(4)\\n# train the model\\ntrain_model(train_dl, model)\\n# evaluate the model\\nacc = evaluate_model(test_dl, model)\\nprint('Accuracy: %.3f' % acc)\\n# make a single prediction\\nrow = [5.1,3.5,1.4,0.2]\\nyhat = predict(row, model)\\nprint('Predicted: %s (class=%d)' % (yhat, argmax(yhat)))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS4_sXtPQNsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}